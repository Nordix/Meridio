#! /bin/sh
##
## forwarder-test_test --
##
##   Test script for forwarder-test executed in xcluster.
##
## Commands;
##

prg=$(basename $0)
dir=$(dirname $0); dir=$(readlink -f $dir)
me=$dir/$prg
tmp=/tmp/${prg}_$$
test -n "$DOMAIN" || DOMAIN=xcluster
yamld=/etc/kubernetes/forwarder-test
ipv6prefix=1000::1

die() {
	echo "ERROR: $*" >&2
	rm -rf $tmp
	exit 1
}
help() {
	grep '^##' $0 | cut -c3-
	rm -rf $tmp
	exit 0
}
test -n "$1" || help
echo "$1" | grep -qi "^help\|-h" && help

log() {
	echo "$prg: $*" >&2
}
dbg() {
	test -n "$__verbose" && echo "$prg: $*" >&2
}

cmd_tcase_check_namespaces() {
	test_namespaces
	tlog "$(kubectl version --short=true | grep Server)"
}
cmd_tcase_check_nodes() {
	test_nodes
}
cmd_tcase_vip_routes() {
	tcase "Set VIP routes (ECMP)"
	vip_route
}

cmd_tcase_multus_setup() {
	tcase "Setup the Multus CRD and default bridge net"
	local multusd=/etc/kubernetes/multus
	kubectl apply -f $multusd/multus-crd.yaml || tdie
	kubectl -n kube-system apply -f $multusd/multus-crd-bridge.yaml || tdie
}

configure_trench() {
	test -n "$1" || tdie "No trench"
	local f=$yamld/$1.conf
	test -r $f || tdie "Not readable [$f]"
	. $f
	test "$NS" != "default" && kubectl="kubectl -n $NS"
}

cmd_tcase_trench() {
	local p
	test "$__local" = "yes" && p=-local
	tcase "Start trench [$1] ($p)"
	configure_trench $1
	if test "$NS" != "default"; then
		kubectl create namespace $NS || tdie "create namespace"
		envsubst < $yamld/spire-template.yaml | kubectl apply -f - || tdie
	fi
	envsubst < $yamld$p/trench-base.yaml | $kubectl apply -f - || tdie
	envsubst < $yamld/conf-template.yaml | $kubectl apply -f - || tdie
	envsubst < $yamld/nse-template.yaml | $kubectl apply -f - || tdie
	envsubst < $yamld$p/load-balancer-template.yaml | $kubectl apply -f - || tdie
	envsubst < $yamld$p/proxy-template.yaml | $kubectl apply -f - || tdie
	test_deployment meridio-nse 120 || tdie
	test_statefulset meridio-ipam 120
	test_statefulset meridio-nsp 30
	test_deployment meridio-load-balancer 30
	test_daemonset meridio-proxy 30
	envsubst < $yamld$p/target-template.yaml | $kubectl apply -f - || tdie
	test_deployment meridio-app 60
}

cmd_tcase_trench_multus() {
	local p
	test "$__local" = "yes" && p=-local
	tcase "Start trench using Multus [$1] ($p)"
	configure_trench $1
	if test "$NS" != "default"; then
		kubectl create namespace $NS || tdie "create namespace"
		kubectl apply -f /etc/kubernetes/multus/crd-$NAME.yaml
	fi
	envsubst < $yamld/spire-template.yaml | kubectl apply -f - || tdie
	envsubst < $yamld$p/trench-base.yaml | $kubectl apply -f - || tdie
	envsubst < $yamld/conf-template.yaml | $kubectl apply -f - || tdie
	envsubst < $yamld$p/load-balancer-multus-template.yaml | $kubectl apply -f - || tdie
	envsubst < $yamld$p/proxy-template.yaml | $kubectl apply -f - || tdie
	test_statefulset meridio-ipam 120
	test_statefulset meridio-nsp 30
	test_deployment meridio-load-balancer 30
	test_daemonset meridio-proxy 30
	envsubst < $yamld$p/target-template.yaml | $kubectl apply -f - || tdie
	test_deployment meridio-app 60
}

cmd_tcase_nsm() {
	tcase "Start nse/nsc in [$1]"
	configure_trench $1
	if test "$NS" != "default"; then
		kubectl create namespace $NS || tdie "create namespace"
		kubectl="kubectl -n $NS"
	fi
	envsubst < $yamld/nse-template.yaml | $kubectl apply -f - || tdie
	test_deployment meridio-nse 120 || tdie
	envsubst < $yamld/nsc-template.yaml | $kubectl apply -f - || tdie
	test_daemonset vlan-nsc 120 || tdie
}

cmd_tcase_multus() {
	tcase "Start an Alpine POD with Multus interfaces in [$1]"
	test -n "$1" || tdie "No namespace"
	export NAME=$1
	local multusd=/etc/kubernetes/multus
	kubectl apply -f $multusd/crd-$1.yaml
	if test "$NAME" != "default"; then
		kubectl create namespace $NAME || tdie "create namespace"
		kubectl="kubectl -n $NAME"
	fi
	envsubst < $multusd/alpine-template.yaml | $kubectl apply -f - || tdie
	test_daemonset alpine-$NAME 120 || tdie
}
cmd_tcase_collect_alpine_addresses() {
	tcase "Collect nsm-1 addresses from Alpine in namespace [$1]"
	test -n "$1" || die 'No ns'
	local ns=$1
	local out=/tmp/alpine-$ns
	local pod
	for pod in $(kubectl get pod -n $ns -l app=alpine-$ns -o name); do
		tex kubectl exec -n $ns $pod -- ip addr show nsm-1 || tdie "No nsm-1 in $pod"
		kubectl exec -n $ns $pod -- ip addr show nsm-1 >> $out-raw
	done
	cat $out-raw
	grep -o -E 'inet 169[0-9\.]+' $out-raw | cut -d' ' -f2 >> $out-ip4
	grep -o -E "inet6 $ipv6prefix:[0-9a-f:]+" $out-raw | cut -d' ' -f2 >> $out-ip6
	cat $out-ip4 $out-ip6 1>&2
}

cmd_tcase_ping_alpine_addresses() {
	tcase "Ping Alpine nsm-1 addresses for namespace [$1]"
	local ns=$1
	local out=/tmp/alpine-$ns
	local ip
	test -r "$out-ip4" || tdie "Not readable [$out-ip4]"
	for ip in $(cat $out-ip4); do
		ping -c1 -W1 $ip || tdie "Ping $ip"
	done
	test -r "$out-ip6" || tdie "Not readable [$out-ip6]"
	for ip in $(cat $out-ip6); do
		ping -c1 -W1 $ip || tdie "Ping $ip"
	done
}

cmd_tcase_setup_vlan() {
	local iface=eth3
	test -n "$1" && iface=$1
	test -n "$__tag" || __tag=100
    tcase "Setup vlan on interface [$iface.$__tag]"
    # Note that eth2 on the VMs is eth3 on router vm-202
	ip link show $iface.$__tag 2>&1 && return 0
	ip link show $iface 2>&1 || tdie "invalid iface $iface"

    echo 0 > /proc/sys/net/ipv6/conf/$iface/accept_dad
    ip link set up $iface
    ethtool -K $iface tx off
    ip link add link $iface name $iface.$__tag type vlan id $__tag
    echo 0 > /proc/sys/net/ipv6/conf/$iface.$__tag/accept_dad
    ip link set up dev $iface.$__tag
	local ipv4
	case $iface.$__tag in
		eth3.100) ipv4=169.254.101.254;;
		eth3.200) ipv4=169.254.102.254;;
		eth4.100) ipv4=169.254.103.254;;
		*) die "Invalid interface [$iface.$__tag]";;
	esac
	#log "Address; $ipv4"
    ip addr add $ipv4/24 dev $iface.$__tag || tdie "ip addr"
    ip -6 addr add $ipv6prefix:$ipv4/120 dev $iface.$__tag || tdie "ip -6 addr"
}

cmd_tcase_local_vlan() {
	test -n "$1" || die "No interface"
	local iface=$1
	test -n "$__tag" || __tag=100
    tcase "Setup vlan on local interface [$iface.$__tag]"
    # Note that eth2 on the VMs is eth3 on router vm-202
	ip link show $iface.$__tag 2>&1 && return 0
	ip link show $iface 2>&1 || tdie "invalid iface $iface"

    echo 0 > /proc/sys/net/ipv6/conf/$iface/accept_dad
    ip link set up $iface
    ethtool -K $iface tx off
    ip link add link $iface name $iface.$__tag type vlan id $__tag
    echo 0 > /proc/sys/net/ipv6/conf/$iface.$__tag/accept_dad
    ip link set up dev $iface.$__tag
}

cmd_tcase_collect_nsc_addresses() {
	tcase "Collect NSC addresses for namespace [$1]"
	test -n "$1" || die 'No ns'
	local ns=$1
	local out=/tmp/nsc-$ns
	local pod
	for pod in $(kubectl get pod -n $ns -l app=vlan-nsc -o name); do
		tex kubectl exec -n $ns $pod -- ip addr show nsm-1 || tdie "No nsm-1 in $pod"
		kubectl exec -n $ns $pod -- ip addr show nsm-1 >> $out-raw
	done
	cat $out-raw
	grep -o -E 'inet 169[0-9\.]+' $out-raw | cut -d' ' -f2 >> $out-ip4
	grep -o -E "inet6 $ipv6prefix:[0-9a-f:]+" $out-raw | cut -d' ' -f2 >> $out-ip6
	cat $out-ip4 $out-ip6 1>&2
}

cmd_tcase_ping_nsc_addresses() {
	tcase "Ping NSC addresses for namespace [$1]"
	local ns=$1
	local out=/tmp/nsc-$ns
	local ip
	test -r "$out-ip4" || tdie "Not readable [$out-ip4]"
	for ip in $(cat $out-ip4); do
		ping -c1 -W1 $ip || tdie "Ping $ip"
	done
	test -r "$out-ip6" || tdie "Not readable [$out-ip6]"
	for ip in $(cat $out-ip6); do
		ping -c1 -W1 $ip || tdie "Ping $ip"
	done
}

cmd_tcase_collect_lb_addresses() {
	tcase "Collect load-balancer addresses for trench [$1]"
	configure_trench $1
	local out=/tmp/meridio-load-balancer-$NAME
	local pod
	for pod in $(kubectl get pod -n $NS -l app=meridio-load-balancer -o name); do
		pushv 60
		tex kubectl exec -n $NS $pod -c load-balancer -- ip addr show nsm-1 || die nsm-1
		popv
		kubectl exec -n $NS $pod -c load-balancer -- ip addr show nsm-1 >> $out-raw
	done
	cat $out-raw
	grep -o -E 'inet 169[0-9\.]+' $out-raw | cut -d' ' -f2 >> $out-ip4
	grep -o -E "inet6 $ipv6prefix:[0-9a-f:]+" $out-raw | cut -d' ' -f2 >> $out-ip6
	cat $out-ip4 $out-ip6 1>&2
}

cmd_tcase_trench_vip_route() {
	tcase "Set VIP routes for trench [$1]"
	test -n "$1" || die 'No trench'
	# Remove old vip routes just in case
	ip ro del 10.0.0.0/24 2> /dev/null
	ip -6 ro del 1000::/120 2> /dev/null
	ip -6 ro del 1000::1:10.0.0.0/120 2> /dev/null

	local ns=$1
	local out=/tmp/meridio-load-balancer-$ns
	local vip4
	case $ns in
		red) vip4=10.0.0.1;;
		blue) vip4=10.0.0.2;;
		green) vip4=10.0.0.3;;
		*) die "Unknown trench";;
	esac
	local hops ip
	for ip in $(cat $out-ip4); do
		hops="$hops nexthop via $ip"
	done
	ip route add $vip4 $hops || die "ip route"
	hops=''
	for ip in $(cat $out-ip6); do
		hops="$hops nexthop via $ip"
	done
	ip -6 route add $ipv6prefix:$vip4 $hops || die "ip -6 route"
}

cmd_tcase_mconnect() {
	tcase "Test mconnect on trench [$1]"
	test -n "$1" || die 'No trench'
	local ns=$1
	local vip4
	case $ns in
		red) vip4=10.0.0.1;;
		blue) vip4=10.0.0.2;;
		green) vip4=10.0.0.3;;
		*) die "Unknown trench";;
	esac
	# timeout retries interval
	conntrack -F > /dev/null 2>&1
	pushv 60 10 5
	local begin=$(date +%s)
	tex "do_mconnect $vip4:5001" || tdie "$vip4:5001"
	tex "do_mconnect [$ipv6prefix:$vip4]:5001" || tdie "[$ipv6prefix:$vip4]:5001"
	popv
	local end=$(date +%s)
	tlog "Mconnect on [$1] succeful after $((end - begin)) sec"
}

cmd_tcase_scale() {
	local replicas=4
	test -n "$2" && replicas=$2
	tcase "Scale application in trench [$1], replicas $replicas"
	configure_trench $1
	$kubectl scale deployment --replicas=$replicas meridio-app || tdie scale
	test_deployment meridio-app 60
}

cmd_tcase_check_targets() {
	local targets=4
	test -n "$2" && targets=$2
	tcase "Check targets in trench [$1], expected $targets"
	configure_trench $1
	tex "target_check $targets" || tdie "targets $(target_count)"
}
target_count() {
	local pod=$($kubectl get pods -l app=meridio-load-balancer -o name | head -1)
	$kubectl exec -c load-balancer $pod -- nfqlb show --shm=tshm-stream1 | grep Active: | tr -cd '(' | wc -c
}
target_check() {
	test $(target_count) -eq $1
}

cmd_tcase_disconnect_targets() {
	tcase "Disconnect $2 targets in trench [$1]"
	test -n "$2" || tdie "No count"
	configure_trench $1
	local cnt=$2
	local f=/tmp/targets
	$kubectl get pods -l app=meridio-app -o name | head -$cnt | shuf > $f
	local pod
	for pod in $(cat $f); do
		tlog "Disconnect $pod"
		$kubectl exec $pod -c meridio-app -- sh -c 'target-client close -c $NSM_SERVICE -t $TRENCH -s $STREAM' || tdie exec
	done
}
cmd_tcase_reconnect_targets() {
	local f=/tmp/targets
	local cnt=$(cat $f | wc -l)
	tcase "Reconnect $cnt targets in trench [$1]"
	configure_trench $1
	local pod
	for pod in $(cat $f); do
		tlog "Disconnect $pod"
		$kubectl exec $pod -c meridio-app -- sh -c 'target-client open -c $NSM_SERVICE -t $TRENCH -s $STREAM' || tdie exec
	done
}

cmd_tcase_check_connections() {
	local targets=4
	test -n "$2" && targets=$2
	tcase "Check connects to trench [$1], expected targets $targets"
	configure_trench $1
	local vip4
	case $NAME in
		red) vip4=10.0.0.1;;
		blue) vip4=10.0.0.2;;
		green) vip4=10.0.0.3;;
		*) die "Unknown trench";;
	esac
	# timeout retries interval
	pushv 120 12 10
	tex "check_connections $vip4 $targets" || tdie $ipv4
	tex "check_connections [$ipv6prefix:$vip4] $targets" || tdie $ipv6prefix:$ipv4
	popv
}
check_connections() {
	mkdir -p $tmp
	local out=$tmp/out
	conntrack -F > /dev/null 2>&1
	if ! mconnect -address $1:5001 -nconn 120 -output json > $out; then
		cat $out | jq
		return 1
	fi
	cat $out | jq
	local v
	for v in failed_connects failed_reads; do
		v=$(cat $out | jq .$v)
		test $v -eq 0 || return 1
	done
	v=$(cat $out | jq '.hosts|length')
	test $v -eq $2 || return 1
	return 0
}

cmd_tcase_conntrack() {
	tcase "Set conntrack size to $1"
	echo $1 > /proc/sys/net/nf_conntrack_max
}

. /etc/profile
. /usr/lib/xctest
indent='  '


# Get the command
cmd=$1
shift
grep -q "^cmd_$cmd()" $0 || die "Invalid command [$cmd]"

while echo "$1" | grep -q '^--'; do
	if echo $1 | grep -q =; then
		o=$(echo "$1" | cut -d= -f1 | sed -e 's,-,_,g')
		v=$(echo "$1" | cut -d= -f2-)
		eval "$o=\"$v\""
	else
		o=$(echo "$1" | sed -e 's,-,_,g')
		eval "$o=yes"
	fi
	shift
done
unset o v
long_opts=`set | grep '^__' | cut -d= -f1`

# Execute command
trap "die Interrupted" INT TERM
cmd_$cmd "$@"
status=$?
rm -rf $tmp
exit $status
